```{r}
library(tidyverse)
library(tidymodels)
library(e1071)
library(ROCR)
```

```{r}
parole <- read_csv("~/Library/CloudStorage/OneDrive-Personal/Documents/UNCW classes/BAN 502 Predictive Analytics/Module 3/Module3Quiz3/parole.csv")
```
Factor conversion
Carefully convert the male, race, state, crime, multiple.offenses, and violator variables to
factors. Recode (rename) the factor levels of each of these variables according to the description of the
variables provided in the ParoleData.txt file (located with the assignment on Canvas). Take your time and
double-check that you have correctly converted and renamed the variables listed above
```{r}
parole <- parole %>% mutate(male = as_factor(male)) %>% 
  mutate(male = fct_recode(male, "female" = "0", "male" = "1" )) 

parole <- parole %>% mutate(race = as_factor(race)) %>% 
  mutate(race = fct_recode(race, "white" = "1", "other" = "2" )) 

parole <- parole %>% mutate(state = as_factor(state)) %>% 
  mutate(state = fct_recode(state, "other" = "1", "Kentucky" = "2", "Louisiana" = "3", "Virginia" = "4" ))

parole <- parole %>% mutate(crime = as_factor(crime)) %>% 
  mutate(crime = fct_recode(crime, "other" = "1", "larceny" = "2", "drug" = "3", "driving" = "4" ))

parole <- parole %>% mutate(multiple.offenses = as_factor(multiple.offenses)) %>% 
  mutate(multiple.offenses = fct_recode(multiple.offenses, "other" = "0", "multiple" = "1" ))

parole <- parole %>% mutate(violator = as_factor(violator)) %>% 
  mutate(violator = fct_recode(violator, "no" = "0", "yes" = "1" ))

str(parole)
```
```{r}
summary(parole)
```

```{r}
set.seed(12345)
parole_split = initial_split(parole, prop = 0.70, strata = violator)
train = training(parole_split)
test = testing(parole_split)
```

```{r}
train = train %>% mutate(violator = fct_relevel(violator, c("no","yes")))
levels(train$violator)
```

```{r}
ggplot(train,aes(x=male,fill = violator)) + geom_bar() + 
  theme_bw()
```

```{r}
t1 = table(train$violator,train$male)
prop.table(t1, margin = 2)
```

```{r}
ggplot(train,aes(x=state,fill = violator)) + geom_bar() + 
  theme_bw()
```

```{r}
ggplot(train,aes(x=max.sentence,fill = violator)) + geom_bar() + 
  theme_bw()
```

```{r}
parole_model = 
  logistic_reg(mode = "classification") %>% #note the use of logistic_reg and mode = "classification"
  set_engine("glm") #standard logistic regression engine is glm

parole_recipe = recipe(violator ~ state, train)

logreg_wf = workflow() %>%
  add_recipe(parole_recipe) %>% 
  add_model(parole_model)

parole_fit = fit(logreg_wf, train)
```

```{r}
summary(parole_fit$fit$fit$fit)
```

```{r}
parole_model2 = 
  logistic_reg(mode = "classification") %>% #note the use of logistic_reg and mode = "classification"
  set_engine("glm") #standard logistic regression engine is glm

parole_recipe2 = recipe(violator ~ state + multiple.offenses + race, train)

logreg_wf = workflow() %>%
  add_recipe(parole_recipe2) %>% 
  add_model(parole_model2)

parole_fit2 = fit(logreg_wf, train)
```

```{r}
summary(parole_fit2$fit$fit$fit)
```
Predictions on sample parolee  
```{r}
newdata = data.frame(state = "Louisiana", multiple.offenses = "multiple", race = "white")
predict(parole_fit2, newdata, type="prob")
```


```{r}
t1 = table(train$violator,train$state,train$multiple.offenses,train$race)
prop.table(t1, margin = 2)
```
Develop predictions  
```{r}
predictions = predict(parole_fit2, train, type="prob") #develop predicted probabilities
head(predictions)
```
Let's extract just the "Yes" prediction.  
```{r}
predictions = predict(parole_fit2, train, type="prob")[2]
head(predictions)
```

Threshold selection  
```{r}
#Change this next line to the names of your predictions and the response variable in the training data frame
ROCRpred = prediction(predictions, train$violator) 

###You shouldn't need to ever change the next two lines:
ROCRperf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
```

Area under the curve (AUC). AUC is a measure of the strength of the model. Values closer to 1 are better. Can be used to compare models.  
```{r}
as.numeric(performance(ROCRpred, "auc")@y.values)
```

```{r}
#Determine threshold to balance sensitivity and specificity
#DO NOT modify this code
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}
print(opt.cut(ROCRperf, ROCRpred))
```

Test thresholds to evaluate accuracy  
```{r}
#confusion matrix
#The "No" and "Yes" represent the actual values
#The "FALSE" and "TRUE" represent our predicted values
t1 = table(train$violator,predictions > 0.2015788)
t1
```

Calculate accuracy  
```{r}
(t1[1,1]+t1[2,2])/nrow(train)
```

Sensitivity
```{r}
36/(36+18)
```

```{r}
parole_model3 = 
  logistic_reg(mode = "classification") %>% #note the use of logistic_reg and mode = "classification"
  set_engine("glm") #standard logistic regression engine is glm

parole_recipe3 = recipe(violator ~ state + multiple.offenses + race, test)

logreg_wf = workflow() %>%
  add_recipe(parole_recipe3) %>% 
  add_model(parole_model3)

parole_fit3 = fit(logreg_wf, train)
```

```{r}
summary(parole_fit3$fit$fit$fit)
```
```{r}
t1 = table(test$violator,test$state,test$multiple.offenses,test$race)
prop.table(t1, margin = 2)
```
Develop predictions  
```{r}
predictions = predict(parole_fit3, test, type="prob") #develop predicted probabilities
head(predictions)
```
Let's extract just the "Yes" prediction.  
```{r}
predictions = predict(parole_fit3, test, type="prob")[2]
head(predictions)
```

Threshold selection  
```{r}
#Change this next line to the names of your predictions and the response variable in the training data frame
ROCRpred = prediction(predictions, test$violator) 

###You shouldn't need to ever change the next two lines:
ROCRperf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
```

Area under the curve (AUC). AUC is a measure of the strength of the model. Values closer to 1 are better. Can be used to compare models.  
```{r}
as.numeric(performance(ROCRpred, "auc")@y.values)
```

```{r}
#Determine threshold to balance sensitivity and specificity
#DO NOT modify this code
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}
print(opt.cut(ROCRperf, ROCRpred))
```

Test thresholds to evaluate accuracy  
```{r}
#confusion matrix
#The "No" and "Yes" represent the actual values
#The "FALSE" and "TRUE" represent our predicted values
t1 = table(test$violator,predictions > 0.5)
t1
```

Calculate accuracy  
```{r}
(t1[1,1]+t1[2,2])/nrow(test)
```
